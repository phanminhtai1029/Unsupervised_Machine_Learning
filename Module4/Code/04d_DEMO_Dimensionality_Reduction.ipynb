{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3dfa20d-0ff4-474d-aa7f-472f66198633",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"cognitiveclass.ai logo\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ee1e68-4393-4103-95a6-a4ac4011604b",
   "metadata": {},
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 4, Part d: Dimensionality Reduction DEMO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84f09a8-1c10-452a-9031-82d5c530474f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We will be using customer data from a [Portuguese wholesale distributor](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers) for clustering. This data file is called `Wholesale_Customers_Data`.\n",
    "\n",
    "It contains the following features:\n",
    "\n",
    "* Fresh: annual spending (m.u.) on fresh products\n",
    "* Milk: annual spending (m.u.) on milk products\n",
    "* Grocery: annual spending (m.u.) on grocery products\n",
    "* Frozen: annual spending (m.u.) on frozen products\n",
    "* Detergents_Paper: annual spending (m.u.) on detergents and paper products\n",
    "* Delicatessen: annual spending (m.u.) on delicatessen products\n",
    "* Channel: customer channel (1: hotel/restaurant/cafe or 2: retail)\n",
    "* Region: customer region (1: Lisbon, 2: Porto, 3: Other)\n",
    "\n",
    "In this data, the values for all spending are given in an arbitrary unit (m.u. = monetary unit).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef56165e-72ba-4e4a-872c-578f2795a9f8",
   "metadata": {},
   "outputs": [],
   "source": "# Define a function to suppress warnings\ndef warn(*args, **kwargs):  # *args: variable number of positional arguments, **kwargs: variable number of keyword arguments\n    pass  # Do nothing when warnings are triggered\n\n# Import the warnings module to handle warning messages\nimport warnings\n# Override the default warn function with our custom one that does nothing\nwarnings.warn = warn\n\n# Import necessary libraries for data analysis and visualization\nimport seaborn as sns  # seaborn: statistical data visualization library\nimport pandas as pd    # pandas: data manipulation and analysis library\nimport numpy as np     # numpy: numerical computing library for arrays and mathematical operations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a7d22b-6dc8-418a-8560-d460664dc858",
   "metadata": {},
   "outputs": [],
   "source": "# Import necessary libraries for data analysis and visualization\nimport os                     # os: operating system interface for file and directory operations\nimport pandas as pd           # pandas: data manipulation and analysis library\nimport numpy as np            # numpy: numerical computing library for arrays and mathematical operations\nimport seaborn as sns         # seaborn: statistical data visualization library\nimport matplotlib.pyplot as plt  # matplotlib.pyplot: plotting library for creating visualizations"
  },
  {
   "cell_type": "markdown",
   "id": "1bb87ec2-b198-45d8-9b27-9f45c74d7961",
   "metadata": {},
   "source": [
    "## TASK 1\n",
    "\n",
    " \n",
    "\n",
    "__Task 1 includes Part 1 to Part 4 which covers data acquistion, preprocessing and PCA__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d9bff9-b9a9-4bc8-9b7f-5c99fcf248c3",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "* Import the data and check the data types.\n",
    "* Drop the channel and region columns as they won't be used since we focus on numeric columns for this example.\n",
    "* Convert the remaining columns to floats if necessary.\n",
    "* Copy this version of the data (using the `copy` method) to a variable to preserve it. We will be using it later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24859caf-30ea-464e-a40f-950afd2fa190",
   "metadata": {},
   "outputs": [],
   "source": "# Load the wholesale customers dataset from a URL into a pandas DataFrame\ndata = pd.read_csv(  # pd.read_csv: function to read CSV file into DataFrame\n    'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/data/Wholesale_Customers_Data.csv',  # URL path to the CSV file\n    sep=','  # sep: delimiter to use (comma-separated values)\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da47a17-1dc2-4791-a981-86f84da6240a",
   "metadata": {},
   "outputs": [],
   "source": "# Display the dimensions of the DataFrame (number of rows and columns)\ndata.shape  # Returns tuple: (number of rows, number of columns)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1111eb-8c45-4af0-9092-e5ecb644e672",
   "metadata": {},
   "outputs": [],
   "source": "# Display the first 5 rows of the DataFrame to preview the data\ndata.head()  # head(): returns the first n rows (default n=5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e7f8d6-2ca9-4805-8522-2906371d9676",
   "metadata": {},
   "outputs": [],
   "source": "# Remove the 'Channel' and 'Region' columns from the DataFrame\ndata = data.drop(  # drop(): removes specified labels from rows or columns\n    ['Channel', 'Region'],  # List of column names to drop\n    axis=1  # axis=1: indicates we are dropping columns (axis=0 would be rows)\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c30a4-f18d-43da-9b2e-c04d0150c406",
   "metadata": {},
   "outputs": [],
   "source": "# Display the data types of each column in the DataFrame\ndata.dtypes  # dtypes: returns the data type of each column"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7853f8f-d7d7-4304-b315-b1efe1036d43",
   "metadata": {},
   "outputs": [],
   "source": "# Convert all columns to float64 data type for numerical operations\nfor col in data.columns:  # Iterate through each column name in the DataFrame\n    data[col] = data[col].astype(np.float64)  # astype(): converts the column to specified data type (np.float64: 64-bit floating point)"
  },
  {
   "cell_type": "markdown",
   "id": "11d9a904-077f-4ea5-ac1d-86c5e1d0889f",
   "metadata": {},
   "source": [
    "Preserve the original data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6352708-e035-488f-87ab-ead7e1469b88",
   "metadata": {},
   "outputs": [],
   "source": "# Create a copy of the data to preserve the original preprocessed version\ndata_orig = data.copy()  # copy(): creates a deep copy of the DataFrame (changes to data won't affect data_orig)"
  },
  {
   "cell_type": "markdown",
   "id": "25a4da0b-0a76-4361-b08a-ad1d53e2ec67",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "As with the previous lesson, we need to ensure the data is scaled and (relatively) normally distributed.\n",
    "\n",
    "* Examine the correlation and skew.\n",
    "* Perform any transformations and scale data using your favorite scaling method.\n",
    "* View the pairwise correlation plots of the new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eccdf7-d097-4015-8faf-3de7d136ee5a",
   "metadata": {},
   "outputs": [],
   "source": "# Calculate the correlation matrix to measure relationships between variables\ncorr_mat = data.corr()  # corr(): computes pairwise correlation of columns\n\n# Remove the diagonal values (self-correlation = 1.0) for clearer analysis\nfor x in range(corr_mat.shape[0]):  # Iterate through each row index\n    corr_mat.iloc[x, x] = 0.0  # iloc[row, col]: set diagonal element to 0.0\n    \ncorr_mat  # Display the correlation matrix"
  },
  {
   "cell_type": "markdown",
   "id": "4266528d-bf96-4518-87f7-6e0e34829f43",
   "metadata": {},
   "source": [
    "As before, the two categories with their respective most strongly correlated variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164be791-f9da-40f5-8f5f-a042e1b281b0",
   "metadata": {},
   "outputs": [],
   "source": "# Find the feature with the highest absolute correlation for each column\ncorr_mat.abs().idxmax()  # abs(): gets absolute values of correlations; idxmax(): returns index of maximum value in each column"
  },
  {
   "cell_type": "markdown",
   "id": "9a228d17-3cd1-44c3-b51f-40a9cd46b70a",
   "metadata": {},
   "source": [
    "Examine the skew values and log transform. Looks like all of them need it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a281215-2a74-4da2-a2af-b20b4446bb89",
   "metadata": {},
   "outputs": [],
   "source": "# Calculate skewness values for each column and filter those needing log transformation\nlog_columns = data.skew().sort_values(ascending=False)  # skew(): computes skewness; sort_values(): sorts in descending order\nlog_columns = log_columns.loc[log_columns > 0.75]  # Filter columns with skewness > 0.75 (right-skewed distributions)\n\nlog_columns  # Display the highly skewed columns"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd870481-a73b-4d19-9027-b220fbbf57ba",
   "metadata": {},
   "outputs": [],
   "source": "# Apply log transformation to reduce skewness in the data\nfor col in log_columns.index:  # Iterate through each column that needs transformation\n    data[col] = np.log1p(data[col])  # np.log1p(): computes log(1 + x) to handle zero values safely"
  },
  {
   "cell_type": "markdown",
   "id": "27282b05-7163-44d6-ac01-41127514fbb8",
   "metadata": {},
   "source": [
    "Scale the data again. Let's use `MinMaxScaler` this time just to mix things up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a6a143-0181-4c4c-94a0-61c9f0835027",
   "metadata": {},
   "outputs": [],
   "source": "# Import MinMaxScaler to scale features to a range [0, 1]\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Create a MinMaxScaler object\nmms = MinMaxScaler()  # MinMaxScaler(): scales each feature to a given range (default [0, 1])\n\n# Scale each column independently to the range [0, 1]\nfor col in data.columns:  # Iterate through each column\n    data[col] = mms.fit_transform(data[[col]]).squeeze()  # fit_transform(): fits scaler and transforms data; squeeze(): removes single-dimensional entries"
  },
  {
   "cell_type": "markdown",
   "source": "### MinMax Scaling Formula\n\nThe MinMaxScaler transforms each feature by scaling it to a given range (default [0, 1]):\n\n$$X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n\nWhere:\n- $X$ is the original value\n- $X_{min}$ is the minimum value in the feature\n- $X_{max}$ is the maximum value in the feature\n- $X_{scaled}$ is the scaled value in the range [0, 1]\n\nThis transformation preserves the shape of the original distribution while ensuring all features are on the same scale.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "965f9b43-639c-4ced-96b1-e3b0bff85044",
   "metadata": {},
   "source": [
    "Visualize the relationship between the variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d4c27-5ada-4884-9842-65ef926aa487",
   "metadata": {},
   "outputs": [],
   "source": "# Configure and create a pairplot visualization to show relationships between all features\nsns.set_context('notebook')  # set_context(): sets the plotting context to 'notebook' for appropriate sizing\nsns.set_style('white')  # set_style(): sets the aesthetic style to 'white' (minimal background)\nsns.pairplot(data);  # pairplot(): creates a grid of scatter plots for each pair of features with histograms on diagonal"
  },
  {
   "cell_type": "markdown",
   "id": "c031d92a-af45-48c0-9efe-55f1b51fc4c2",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "In this section, we will:\n",
    "* Using Scikit-learn's [pipeline function](http://scikit-learn.org/stable/modules/pipeline.html), recreate the data pre-processing scheme above (transformation and scaling) using a pipeline. If you used a non-Scikit learn function to transform the data (e.g. NumPy's log function), checkout  the custom transformer class called [`FunctionTransformer`](http://scikit-learn.org/stable/modules/preprocessing.html#custom-transformers).\n",
    "* Use the pipeline to transform the original data that was stored at the end of question 1.\n",
    "* Compare the results to the original data to verify that everything worked.\n",
    "\n",
    "*Note:* Scikit-learn has a more flexible `Pipeline` function and a shortcut version called `make_pipeline`. Either can be used. Also, if different transformations need to be performed on the data, a [`FeatureUnion`](http://scikit-learn.org/stable/modules/pipeline.html#featureunion-composite-feature-spaces) can be used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b170a9-badc-43e3-8669-2f041d94f891",
   "metadata": {},
   "outputs": [],
   "source": "# Import necessary classes for creating a preprocessing pipeline\nfrom sklearn.preprocessing import FunctionTransformer  # For creating custom transformers from functions\nfrom sklearn.pipeline import Pipeline  # For chaining multiple preprocessing steps\n\n# Create a custom transformer that applies numpy's log1p function\nlog_transformer = FunctionTransformer(np.log1p)  # FunctionTransformer(): wraps np.log1p function for use in pipeline\n\n# Define the pipeline steps as a list of (name, transformer) tuples\nestimators = [\n    ('log1p', log_transformer),  # Step 1: apply log transformation\n    ('minmaxscale', MinMaxScaler())  # Step 2: apply MinMax scaling\n]\n# Create the pipeline by chaining the transformers\npipeline = Pipeline(estimators)  # Pipeline(): chains multiple transformers together\n\n# Apply the entire pipeline to the original data\ndata_pipe = pipeline.fit_transform(data_orig)  # fit_transform(): fits the pipeline and transforms the data in one step"
  },
  {
   "cell_type": "markdown",
   "id": "3744dbbd-ea94-4bb5-bb46-92ce97e61f64",
   "metadata": {},
   "source": [
    "The results are identical. Note that machine learning models and grid searches can also be added to the pipeline (and in fact, usually are.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1062048-7f36-4275-b63e-88987d3ee073",
   "metadata": {},
   "outputs": [],
   "source": "# Verify that the pipeline produces identical results to manual transformation\nnp.allclose(data_pipe, data)  # allclose(): returns True if two arrays are element-wise equal within a tolerance"
  },
  {
   "cell_type": "markdown",
   "id": "98839eb1-8599-4bcf-86b1-d640d9475b0a",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "\n",
    "In this section, we will:\n",
    "* Perform PCA with `n_components` ranging from 1 to 5. \n",
    "* Store the amount of explained variance for each number of dimensions.\n",
    "* Also store the feature importance for each number of dimensions. *Hint:* PCA doesn't explicitly provide this after a model is fit, but the `components_` properties can be used to determine something that approximates importance. How you decided to do so is entirely up to you.\n",
    "* Plot the explained variance and feature importances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a86489f-521b-45f8-aeab-dd8e2718eb00",
   "metadata": {},
   "outputs": [],
   "source": "# Import PCA for dimensionality reduction\nfrom sklearn.decomposition import PCA\n\n# Initialize lists to store PCA models and feature importance weights\npca_list = list()\nfeature_weight_list = list()\n\n# Fit PCA models with different numbers of components (1 to 5)\nfor n in range(1, 6):  # n: number of principal components to keep\n    \n    # Create and fit a PCA model with n components\n    PCAmod = PCA(n_components=n)  # PCA(): creates a PCA model; n_components: number of components to keep\n    PCAmod.fit(data)  # fit(): computes principal components from the data\n    \n    # Store the model along with total explained variance\n    pca_list.append(pd.Series({\n        'n': n,  # Number of components\n        'model': PCAmod,  # Fitted PCA model\n        'var': PCAmod.explained_variance_ratio_.sum()  # explained_variance_ratio_: proportion of variance explained by each component\n    }))\n    \n    # Calculate feature importance by summing absolute component values\n    abs_feature_values = np.abs(PCAmod.components_).sum(axis=0)  # components_: principal axes in feature space; sum(axis=0): sum across components\n    feature_weight_list.append(pd.DataFrame({\n        'n': n,  # Number of components\n        'features': data.columns,  # Feature names\n        'values': abs_feature_values / abs_feature_values.sum()  # Normalized feature importance\n    }))\n    \n# Combine all PCA results into a DataFrame\npca_df = pd.concat(pca_list, axis=1).T.set_index('n')  # concat(): combines Series; T: transposes; set_index(): sets 'n' as index\npca_df"
  },
  {
   "cell_type": "markdown",
   "source": "### Principal Component Analysis (PCA) Formulas\n\nPCA transforms the data into a new coordinate system where the greatest variance lies on the first coordinate (first principal component), the second greatest variance on the second coordinate, and so on.\n\n**1. Data Centering:**\n$$X_{centered} = X - \\mu$$\n\nWhere $\\mu$ is the mean of each feature.\n\n**2. Covariance Matrix:**\n$$C = \\frac{1}{n-1} X_{centered}^T X_{centered}$$\n\nWhere $n$ is the number of samples.\n\n**3. Eigendecomposition:**\n$$C v_i = \\lambda_i v_i$$\n\nWhere:\n- $v_i$ are the eigenvectors (principal components)\n- $\\lambda_i$ are the eigenvalues (variance explained by each component)\n\n**4. Projection onto Principal Components:**\n$$Z = X_{centered} \\cdot V_k$$\n\nWhere $V_k$ contains the first $k$ eigenvectors.\n\n**5. Explained Variance Ratio:**\n$$\\text{Explained Variance Ratio}_i = \\frac{\\lambda_i}{\\sum_{j=1}^{p} \\lambda_j}$$\n\nThis represents the proportion of total variance explained by the $i$-th principal component.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "178a777b-80d4-4baf-a404-5bf93d4658b1",
   "metadata": {},
   "source": [
    "Create a table of feature importances for each data column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbd79d-ca79-438c-baa3-b0f0ca544bec",
   "metadata": {},
   "outputs": [],
   "source": "# Create a pivot table showing feature importance across different numbers of components\nfeatures_df = (\n    pd.concat(feature_weight_list)  # Combine all feature weight DataFrames\n    .pivot(  # pivot(): reshapes data (long to wide format)\n        index='n',  # index: rows will be number of components\n        columns='features',  # columns: columns will be feature names\n        values='values'  # values: cell values will be feature importance weights\n    )\n)\n\nfeatures_df  # Display the feature importance table"
  },
  {
   "cell_type": "markdown",
   "id": "67aeac80-4759-4126-a0ef-3a9ff4f2eff9",
   "metadata": {},
   "source": [
    "Create a plot of explained variances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d70c12-fc0a-4b3c-a1cc-ae8deeaf7b1f",
   "metadata": {},
   "outputs": [],
   "source": "# Set larger text size for the plot and create a bar chart of explained variance\nsns.set_context('talk')  # set_context('talk'): increases font sizes for presentations\nax = pca_df['var'].plot(kind='bar')  # plot(): creates a plot; kind='bar': specifies bar chart type\n\n# Configure the plot labels and title\nax.set(\n    xlabel='Number of dimensions',  # Label for x-axis\n    ylabel='Percent explained variance',  # Label for y-axis\n    title='Explained Variance vs Dimensions'  # Chart title\n);"
  },
  {
   "cell_type": "markdown",
   "id": "c88d868f-2aca-4656-87fb-ee82bfcd197f",
   "metadata": {},
   "source": [
    "And here's a plot of feature importances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df60130-ba98-4727-b098-dbf68387d65b",
   "metadata": {},
   "outputs": [],
   "source": "# Create a bar chart showing feature importance for each number of dimensions\nax = features_df.plot(\n    kind='bar',  # kind='bar': creates a grouped bar chart\n    figsize=(13, 8)  # figsize: sets figure size in inches (width, height)\n)\nax.legend(loc='upper right')  # legend(): adds legend; loc: specifies location\n# Configure the plot labels and title\nax.set(\n    xlabel='Number of dimensions',  # Label for x-axis\n    ylabel='Relative importance',  # Label for y-axis\n    title='Feature importance vs Dimensions'  # Chart title\n);"
  },
  {
   "cell_type": "markdown",
   "id": "ab45d762-efc9-4220-ae9f-402bded665fc",
   "metadata": {},
   "source": [
    "This concludes \"Demo lab: Dimensionality Reduction (Part 1)\". We will be going over the remaining parts in the next module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c63541c-751b-4496-b43d-b5b45ebe9b23",
   "metadata": {},
   "source": [
    "## TASK 2\n",
    "\n",
    " \n",
    "\n",
    "__Task 2 covers part 5 and 6. It includes KernelPCA and Model accuracy.__\n",
    "\n",
    " \n",
    "\n",
    ">Note: Task 1 is a pre-requisite for this task. Please make sure you complete Task 1 before proceeding to Task 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f77dee-a3e9-4729-b4b2-111d04734c7d",
   "metadata": {},
   "source": [
    "## Part 5\n",
    "\n",
    "In this section, we will:\n",
    "* Fit a `KernelPCA` model with `kernel='rbf'`. You can choose how many components and what values to use for the other parameters (`rbf` refers to a Radial Basis Function kernel, and the `gamma` parameter governs scaling of this kernel and typically ranges between 0 and 1). Several other [kernels](https://scikit-learn.org/stable/modules/metrics.html) can be tried, and even passed ss cross validation parameters (see this [example](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html)).\n",
    "* If you want to tinker some more, use `GridSearchCV` to tune the parameters of the `KernelPCA` model. \n",
    "\n",
    "The second step is tricky since grid searches are generally used for supervised machine learning methods and rely on scoring metrics, such as accuracy, to determine the best model. However, a custom scoring function can be written for `GridSearchCV`, where larger is better for the outcome of the scoring function. \n",
    "\n",
    "What would such a metric involve for PCA? What about percent of explained variance? Or perhaps the negative mean squared error on the data once it has been transformed and then inversely transformed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00981447-a571-4088-8414-9cf2a8d69bc0",
   "metadata": {},
   "outputs": [],
   "source": "# Import necessary modules for Kernel PCA and hyperparameter tuning\nfrom sklearn.decomposition import KernelPCA  # KernelPCA: non-linear dimensionality reduction using kernels\nfrom sklearn.model_selection import GridSearchCV  # GridSearchCV: exhaustive search over parameter grid\nfrom sklearn.metrics import mean_squared_error  # mean_squared_error: calculates MSE between actual and predicted values\n\n# Define a custom scoring function for GridSearchCV (higher is better)\ndef scorer(pcamodel, X, y=None):  # pcamodel: PCA model to evaluate; X: input data; y: not used (None)\n\n    try:\n        X_val = X.values  # Try to extract numpy array from pandas DataFrame\n    except:\n        X_val = X  # If X is already a numpy array, use it directly\n        \n    # Transform data to lower dimensions and back to original space\n    data_inv = pcamodel.fit(X_val).transform(X_val)  # transform(): projects data onto principal components\n    data_inv = pcamodel.inverse_transform(data_inv)  # inverse_transform(): reconstructs original features from components\n    \n    # Calculate reconstruction error using mean squared error\n    mse = mean_squared_error(data_inv.ravel(), X_val.ravel())  # ravel(): flattens arrays to 1D\n    \n    # Return negative MSE since GridSearchCV maximizes the score (lower MSE is better)\n    return -1.0 * mse\n\n# Define the parameter grid for hyperparameter tuning\nparam_grid = {\n    'gamma': [0.001, 0.01, 0.05, 0.1, 0.5, 1.0],  # gamma: kernel coefficient for RBF (controls kernel width)\n    'n_components': [2, 3, 4]  # n_components: number of components to keep\n}\n\n# Create a GridSearchCV object to find best KernelPCA parameters\nkernelPCA = GridSearchCV(\n    KernelPCA(kernel='rbf', fit_inverse_transform=True),  # kernel='rbf': Radial Basis Function kernel; fit_inverse_transform=True: enables reconstruction\n    param_grid=param_grid,  # Parameter combinations to try\n    scoring=scorer,  # Custom scoring function to optimize\n    n_jobs=-1  # n_jobs=-1: use all available CPU cores for parallel processing\n)\n\n# Fit the grid search to find optimal parameters\nkernelPCA = kernelPCA.fit(data)  # fit(): tries all parameter combinations and finds the best one\n\nkernelPCA.best_estimator_  # Display the best KernelPCA model found"
  },
  {
   "cell_type": "markdown",
   "source": "### Kernel PCA with RBF Kernel Formulas\n\nKernel PCA extends standard PCA to capture non-linear patterns by using the kernel trick.\n\n**1. RBF (Radial Basis Function) Kernel:**\n$$K(x, y) = \\exp\\left(-\\gamma \\|x - y\\|^2\\right)$$\n\nWhere:\n- $x, y$ are data points\n- $\\gamma$ is the kernel coefficient (controls kernel width)\n- $\\|x - y\\|^2$ is the squared Euclidean distance between points\n\n**2. Kernel Matrix:**\n$$K_{ij} = K(x_i, x_j)$$\n\nThe kernel matrix $K$ contains pairwise kernel values for all data points.\n\n**3. Centered Kernel Matrix:**\n$$\\tilde{K} = K - 1_n K - K 1_n + 1_n K 1_n$$\n\nWhere $1_n$ is an $n \\times n$ matrix with all elements equal to $\\frac{1}{n}$.\n\n**4. Eigendecomposition of Centered Kernel Matrix:**\n$$\\tilde{K} \\alpha_i = \\lambda_i \\alpha_i$$\n\nWhere $\\alpha_i$ are the eigenvectors and $\\lambda_i$ are the eigenvalues.\n\n**5. Reconstruction Error (MSE):**\n$$MSE = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\hat{x}_i)^2$$\n\nWhere $x_i$ is the original data and $\\hat{x}_i$ is the reconstructed data after transforming and inverse transforming.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "b3e49b46-8349-4ebf-adb6-f3afea498163",
   "metadata": {},
   "source": [
    "## Part 6\n",
    "\n",
    "Let's explore how our model accuracy may change if we include a `PCA` in our model building pipeline. Let's plan to use sklearn's `Pipeline` class and create a pipeline that has the following steps:\n",
    "<ol>\n",
    "  <li>A scaler</li>\n",
    "  <li>`PCA(n_components=n)`</li>\n",
    "  <li>`LogisticRegression`</li>\n",
    "</ol>\n",
    "\n",
    "* Load the Human Activity data from the datasets.\n",
    "* Write a function that takes in a value of `n` and makes the above pipeline, then predicts the \"Activity\" column over a 5-fold StratifiedShuffleSplit, and returns the average test accuracy\n",
    "* For various values of n, call the above function and store the average accuracies.\n",
    "* Plot the average accuracy by number of dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d841e99f-15b0-40c8-bbfe-3e8523484b1d",
   "metadata": {},
   "outputs": [],
   "source": "# Load the Human Activity Recognition dataset from a URL\ndata = pd.read_csv(  # pd.read_csv: function to read CSV file into DataFrame\n    'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/Human_Activity_Recognition_Using_Smartphones_Data.csv',  # URL path to the dataset\n    sep=','  # sep: delimiter to use (comma-separated values)\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44cf322-af4b-4272-bec2-f7b3bcdd6d5a",
   "metadata": {},
   "outputs": [],
   "source": "# Display all column names in the dataset\ndata.columns  # columns: returns the column labels of the DataFrame"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb860af-3534-4f5e-b254-f863926fa8e3",
   "metadata": {},
   "outputs": [],
   "source": "# Import necessary modules for building and evaluating a machine learning pipeline\nfrom sklearn.pipeline import Pipeline  # Pipeline: chains preprocessing and model steps\nfrom sklearn.preprocessing import StandardScaler  # StandardScaler: standardizes features by removing mean and scaling to unit variance\nfrom sklearn.model_selection import StratifiedShuffleSplit  # StratifiedShuffleSplit: creates stratified train/test splits\nfrom sklearn.linear_model import LogisticRegression  # LogisticRegression: classification model\nfrom sklearn.metrics import accuracy_score  # accuracy_score: calculates classification accuracy\n\n# Separate features (X) and target variable (y)\nX = data.drop('Activity', axis=1)  # Drop the target column; axis=1: drop column\ny = data.Activity  # Extract the Activity column as target\n\n# Create a stratified cross-validation splitter\nsss = StratifiedShuffleSplit(n_splits=5, random_state=42)  # n_splits=5: create 5 train/test splits; random_state=42: ensures reproducibility\n\n# Define function to evaluate average accuracy for a given number of PCA components\ndef get_avg_score(n):  # n: number of PCA components to use\n    # Define the pipeline steps\n    pipe = [\n        ('scaler', StandardScaler()),  # Step 1: standardize features (zero mean, unit variance)\n        ('pca', PCA(n_components=n)),  # Step 2: reduce dimensionality to n components\n        ('estimator', LogisticRegression(solver='liblinear'))  # Step 3: train logistic regression; solver='liblinear': optimization algorithm\n    ]\n    pipe = Pipeline(pipe)  # Create the pipeline\n    scores = []  # List to store accuracy scores for each fold\n    \n    # Iterate through each train/test split\n    for train_index, test_index in sss.split(X, y):  # split(): generates train/test indices\n        X_train, X_test = X.loc[train_index], X.loc[test_index]  # Split features\n        y_train, y_test = y.loc[train_index], y.loc[test_index]  # Split target\n        pipe.fit(X_train, y_train)  # fit(): train the entire pipeline on training data\n        scores.append(accuracy_score(y_test, pipe.predict(X_test)))  # predict(): make predictions; accuracy_score(): calculate accuracy\n    \n    return np.mean(scores)  # Return average accuracy across all folds\n\n# Test different numbers of PCA components\nns = [10, 20, 50, 100, 150, 200, 300, 400]  # List of component numbers to try\nscore_list = [get_avg_score(n) for n in ns]  # Calculate average accuracy for each n"
  },
  {
   "cell_type": "markdown",
   "source": "### Machine Learning Pipeline Formulas\n\nThis pipeline combines StandardScaler, PCA, and Logistic Regression.\n\n**1. Standard Scaling:**\n$$z = \\frac{x - \\mu}{\\sigma}$$\n\nWhere:\n- $x$ is the original feature value\n- $\\mu$ is the mean of the feature\n- $\\sigma$ is the standard deviation of the feature\n- $z$ is the standardized value\n\n**2. Logistic Regression:**\n\nThe logistic regression model predicts probabilities using the sigmoid function:\n\n$$P(y=1|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n)}}$$\n\nOr in matrix form:\n$$P(y=1|x) = \\frac{1}{1 + e^{-\\beta^T x}}$$\n\n**3. Cross-Entropy Loss Function:**\n$$J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[y_i \\log(h_\\beta(x_i)) + (1-y_i) \\log(1-h_\\beta(x_i))\\right]$$\n\nWhere:\n- $m$ is the number of training examples\n- $y_i$ is the actual label (0 or 1)\n- $h_\\beta(x_i)$ is the predicted probability\n\n**4. Accuracy Score:**\n$$\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$$",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f11e6e9-e89f-445c-aa2b-ca48b056581f",
   "metadata": {},
   "outputs": [],
   "source": "# Set larger text size for the plot\nsns.set_context('talk')  # set_context('talk'): increases font sizes for presentations\n\n# Create a matplotlib axes object for the plot\nax = plt.axes()  # axes(): creates a new axes object for plotting\n# Plot accuracy scores vs number of dimensions\nax.plot(ns, score_list)  # plot(): creates a line plot; ns: x-values (number of dimensions); score_list: y-values (accuracy scores)\n\n# Configure the plot with labels and title\nax.set(\n    xlabel='Number of Dimensions',  # Label for x-axis\n    ylabel='Average Accuracy',  # Label for y-axis\n    title='LogisticRegression Accuracy vs Number of dimensions on the Human Activity Dataset'  # Chart title\n)\nax.grid(True)  # grid(): adds a grid to the plot; True: enables grid lines"
  },
  {
   "cell_type": "markdown",
   "id": "bdeac906-e883-452f-b9cd-4e4e9aef9650",
   "metadata": {},
   "source": [
    "---\n",
    "### Machine Learning Foundation (C) 2020 IBM Corporation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}