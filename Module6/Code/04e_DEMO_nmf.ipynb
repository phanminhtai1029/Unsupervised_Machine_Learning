{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"cognitiveclass.ai logo\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 4, Part e: Non-Negative Matrix Factorization DEMO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise illustrates usage of Non-negative Matrix factorization and covers techniques related to sparse matrices and some basic work with Natural Langauge Processing.  We will use NMF to look at the top words for given topics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the BBC dataset. These are articles collected from 5 different topics, with the data pre-processed. \n",
    "\n",
    "These data are available in the data folder (or online [here](http://mlg.ucd.ie/files/datasets/bbc.zip?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork821-2023-01-01)). The data consists of a few files. The steps we'll be following are:\n",
    "\n",
    "* *bbc.terms* is just a list of words \n",
    "* *bbc.docs* is a list of artcles listed by topic.\n",
    "\n",
    "At a high level, we're going to \n",
    "\n",
    "1. Turn the `bbc.mtx` file into a sparse matrix (a [sparse matrix](https://docs.scipy.org/doc/scipy/reference/sparse.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork821-2023-01-01) format can be useful for matrices with many values that are 0, and save space by storing the position and values of non-zero elements).\n",
    "1. Decompose that sparse matrix using NMF.\n",
    "1. Use the resulting components of NMF to analyze the topics that result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This lab has been updated to work in skillsnetwork for your convenience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import urllib library for making HTTP requests\nimport urllib"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Open the URL to read the bbc.mtx file from remote storage\n# urlopen: opens the URL and returns a file-like object\nwith urllib.request.urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/data/bbc.mtx') as r:\n    # Read all lines from the file and skip the first 2 header lines using slice [2:]\n    content = r.readlines()[2:]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "Here, we will turn this into a list of tuples representing a [sparse matrix](https://docs.scipy.org/doc/scipy/reference/sparse.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork821-2023-01-01). Remember the description of the file from above:\n",
    "\n",
    "* *bbc.mtx* is a list: first column is **wordID**, second is **articleID** and the third is the number of times that word appeared in that article.\n",
    "\n",
    "So, if word 1 appears in article 3, 2 times, one element of our list will be:\n",
    "\n",
    "`(1, 3, 2)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert each line into a tuple of integers (wordID, articleID, count)\n# c.split(): splits each line by whitespace\n# map(float, ...): converts split strings to floats first\n# map(int, ...): then converts floats to integers\n# tuple(...): creates a tuple from the three values\nsparsemat = [tuple(map(int,map(float,c.split()))) for c in content]\n# Let's examine the first few elements using slice [:8]\nsparsemat[:8]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preparing Sparse Matrix data for NMF \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [coo matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork821-2023-01-01) function to turn the sparse matrix into an array. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import numpy for numerical operations\nimport numpy as np\n# Import coo_matrix (coordinate format) for creating sparse matrices\nfrom scipy.sparse import coo_matrix\n# Extract row indices (wordID) from each tuple using list comprehension\nrows = [x[0] for x in sparsemat]\n# Extract column indices (articleID) from each tuple\ncols = [x[1] for x in sparsemat]\n# Extract the values (word count) from each tuple\nvalues = [x[2] for x in sparsemat]\n# Create a COO sparse matrix from the extracted data\n# coo_matrix((data, (row_indices, col_indices))): creates sparse matrix with specified values at given positions\ncoo = coo_matrix((values, (rows, cols)))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF is a way of decomposing a matrix of documents and words so that one of the matrices can be interpreted as the \"loadings\" or \"weights\" of each word on a topic. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out [the NMF documentation](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork821-2023-01-01) and the [examples of topic extraction using NMF and LDA](http://scikit-learn.org/0.18/auto_examples/applications/topics_extraction_with_nmf_lda.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork821-2023-01-01).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "Here, we will import `NMF`, define a model object with 5 components, and `fit_transform` the data created above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Suppress warnings from using older version of sklearn\n# Define a custom warn function that does nothing (passes)\ndef warn(*args, **kwargs):\n    pass\n# Import warnings module\nimport warnings\n# Replace the default warn function with our custom one\nwarnings.warn = warn\n\n# Import NMF (Non-negative Matrix Factorization) from sklearn\nfrom sklearn.decomposition import NMF\n# Create NMF model with 5 components (topics)\n# n_components: number of topics to extract (5)\n# init: initialization method ('random' uses random initialization)\n# random_state: seed for reproducibility (818)\nmodel = NMF(n_components=5, init='random', random_state=818)\n# Fit the model to the sparse matrix and transform it to get document-topic matrix\n# fit_transform: learns the model and returns the transformed data\ndoc_topic = model.fit_transform(coo)\n\n# Get the shape of the document-topic matrix\ndoc_topic.shape\n# we should have 9636 observations (articles) and five latent features"
  },
  {
   "cell_type": "markdown",
   "source": "### NMF Formula Explanation\n\nNon-negative Matrix Factorization decomposes the original matrix into two lower-rank matrices:\n\n$$V \\approx W \\times H$$\n\nWhere:\n- $V$ is the original matrix (documents × words) of size $m \\times n$\n- $W$ is the document-topic matrix (documents × topics) of size $m \\times k$\n- $H$ is the topic-word matrix (topics × words) of size $k \\times n$\n- $k$ is the number of components/topics (in our case, 5)\n\n**Objective Function:**\n\nNMF minimizes the reconstruction error using Frobenius norm:\n\n$$\\min_{W,H} \\|V - WH\\|_F^2 \\quad \\text{subject to} \\quad W \\geq 0, H \\geq 0$$\n\nThe Frobenius norm is defined as:\n\n$$\\|V - WH\\|_F = \\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n}(V_{ij} - (WH)_{ij})^2}$$\n\n**Non-negativity Constraint:**\nAll values in $W$ and $H$ must be non-negative ($\\geq 0$), which makes the results interpretable as \"parts-based\" representations where topics are additive combinations of words.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find the topic (feature) with the highest value for each document\n# np.argmax: returns indices of maximum values\n# axis=1: operates along rows (finds max across columns for each row)\nnp.argmax(doc_topic, axis=1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: \n",
    "\n",
    "Check out the `components` of this model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get the shape of the components matrix (topic-word matrix)\n# components_: the learned topic-word weight matrix\nmodel.components_.shape"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is five rows, each of which is a \"topic\" containing the weights of each word on that topic. The exercise is to _get a list of the top 10 words for each topic_. We can just store this in a list of lists.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Just like we read in the data above, we'll have to read in the words from the `bbc.terms` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Open the URL to read the bbc.terms file (list of words)\nwith urllib.request.urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/data/bbc.terms') as r:\n    # Read all lines from the file\n    content = r.readlines()\n# Extract the first word from each line (split by whitespace and take element [0])\nwords = [c.split()[0] for c in content]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize empty list to store top words for each topic\ntopic_words = []\n# Iterate through each row (topic) in the components matrix\nfor r in model.components_:\n    # Create list of (value, index) tuples using enumerate\n    # enumerate(r): pairs each weight with its index\n    # sorted(..., reverse=True): sorts by value in descending order\n    # [0:12]: selects top 12 words with highest weights\n    a = sorted([(v,i) for i,v in enumerate(r)],reverse=True)[0:12]\n    # Extract the actual words using their indices and append to topic_words\n    # e[1]: gets the index from each (value, index) tuple\n    # words[e[1]]: retrieves the word at that index\n    topic_words.append([words[e[1]] for e in a])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display the top words for the first 5 topics using slice [:5]\n# Here, each set of words relates to the corresponding topic (ie the first set of words relates to topic 'Business', etc.)\ntopic_words[:5]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data had 5 topics, as listed in `bbc.docs` (which these topic words relate to). \n",
    "\n",
    "```\n",
    "Business\n",
    "Entertainment\n",
    "Politics\n",
    "Sport\n",
    "Tech\n",
    "```\n",
    "\n",
    "In \"real life\", we would have found a way to use these to inform the model. But for this little demo, we can just compare the recovered topics to the original ones. And they seem to match reasonably well. The order is different, which is to be expected in this kind of model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Open the URL to read the bbc.docs file (document labels)\nwith urllib.request.urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/data/bbc.docs') as r:\n    # Read all lines from the file\n    doc_content = r.readlines()\n\n# Display the first 8 document labels using slice [:8]    \ndoc_content[:8]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Machine Learning Foundation (C) 2020 IBM Corporation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}